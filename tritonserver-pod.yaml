apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  name: triton-inference-server
  namespace: default
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: triton-inference-server
      release: triton-inference-server-1
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
      creationTimestamp: null
      labels:
        app: triton-inference-server
        release: triton-inference-server-1
    spec:
      containers:
      - args:
        - tritonserver
        - --model-store=gs://model-store
        - --strict-model-config=false
        - --log-verbose=1
        - --allow-gpu-metrics=true
        - --model-control-mode=explicit
        env:
        - name: LD_PRELOAD
        - name: GOOGLE_APPLICATION_CREDENTIALS
          value: /secret/key.json
        image: gcr.io/nvidia-ngc-public/tritonserver:22.08-py3
        volumeMounts:
        - name: vsecret
          mountPath: "/secret"
          readOnly: true
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 60
          httpGet:
            path: /v2/health/live
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        name: triton-inference-server
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        - containerPort: 8001
          name: grpc
          protocol: TCP
        - containerPort: 8002
          name: metrics
          protocol: TCP
        readinessProbe:
          failureThreshold: 60
          httpGet:
            path: /v2/health/ready
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            nvidia.com/gpu: "2"
        securityContext:
          runAsUser: 1000
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      volumes:
      - name: vsecret
        secret:
          secretName: gcpcreds
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30

---

apiVersion: v1
kind: Service
metadata:
  name: triton-inference-server
spec:
  selector:
    app: triton-inference-server
  ports:
    - port: 8000
      targetPort: 8001
  type: LoadBalancer
